{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_treinamento = list(x_train)\n",
    "#x_treinamento.append(cv2.imread('um.png',  0)/255)\n",
    "#y_treinamento = list(y_train)\n",
    "#y_treinamento.append(1)\n",
    "#x_train = np.array(x_treinamento)\n",
    "#y_train = np.array(y_treinamento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28).astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.add(Conv2D(15, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax', name='preds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 24, 24)        780       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 10, 10)        4065      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               48128     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 63,639\n",
      "Trainable params: 63,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.4156 - acc: 0.8674 - val_loss: 0.0845 - val_acc: 0.9748\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0970 - acc: 0.9696 - val_loss: 0.0525 - val_acc: 0.9836\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0697 - acc: 0.9781 - val_loss: 0.0396 - val_acc: 0.9874\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0578 - acc: 0.9824 - val_loss: 0.0369 - val_acc: 0.9886\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0470 - acc: 0.9852 - val_loss: 0.0308 - val_acc: 0.9898\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0411 - acc: 0.9869 - val_loss: 0.0251 - val_acc: 0.9915\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0382 - acc: 0.9879 - val_loss: 0.0255 - val_acc: 0.9910\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.0257 - val_acc: 0.9917\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0321 - acc: 0.9895 - val_loss: 0.0224 - val_acc: 0.9928\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0299 - acc: 0.9904 - val_loss: 0.0248 - val_acc: 0.9918\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0241 - val_acc: 0.9926\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.0263 - acc: 0.9914 - val_loss: 0.0249 - val_acc: 0.9915\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0236 - acc: 0.9924 - val_loss: 0.0220 - val_acc: 0.9925\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0222 - acc: 0.9924 - val_loss: 0.0277 - val_acc: 0.9907\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.0211 - acc: 0.9930 - val_loss: 0.0212 - val_acc: 0.9938\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0201 - acc: 0.9934 - val_loss: 0.0284 - val_acc: 0.9917\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0199 - acc: 0.9932 - val_loss: 0.0218 - val_acc: 0.9934\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0174 - acc: 0.9945 - val_loss: 0.0224 - val_acc: 0.9931\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0255 - val_acc: 0.9923\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0159 - acc: 0.9946 - val_loss: 0.0273 - val_acc: 0.9915\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0252 - val_acc: 0.9941\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0153 - acc: 0.9947 - val_loss: 0.0247 - val_acc: 0.9925\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0139 - acc: 0.9953 - val_loss: 0.0224 - val_acc: 0.9938\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0234 - val_acc: 0.9933\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0129 - acc: 0.9957 - val_loss: 0.0296 - val_acc: 0.9921\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0146 - acc: 0.9952 - val_loss: 0.0253 - val_acc: 0.9941\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0288 - val_acc: 0.9924\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0272 - val_acc: 0.9920\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0265 - val_acc: 0.9937\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0247 - val_acc: 0.9934\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0278 - val_acc: 0.9926\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0242 - val_acc: 0.9927\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0263 - val_acc: 0.9931\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0272 - val_acc: 0.9934\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0281 - val_acc: 0.9926\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0292 - val_acc: 0.9931\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0273 - val_acc: 0.9931\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0295 - val_acc: 0.9929\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0093 - acc: 0.9968 - val_loss: 0.0241 - val_acc: 0.9931\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0287 - val_acc: 0.9925\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0274 - val_acc: 0.9940\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0274 - val_acc: 0.9938\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0297 - val_acc: 0.9932\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0310 - val_acc: 0.9925\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0077 - acc: 0.9972 - val_loss: 0.0334 - val_acc: 0.9934\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0302 - val_acc: 0.9935\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0288 - val_acc: 0.9929\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.0303 - val_acc: 0.9919\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0278 - val_acc: 0.9925\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0270 - val_acc: 0.9931\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0426 - val_acc: 0.9912\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0279 - val_acc: 0.9931\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0299 - val_acc: 0.9927\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0333 - val_acc: 0.9925\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0300 - val_acc: 0.9926\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0316 - val_acc: 0.9920\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.0339 - val_acc: 0.9920\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0285 - val_acc: 0.9924\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0285 - val_acc: 0.9937\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0297 - val_acc: 0.9926\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0336 - val_acc: 0.9926\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0325 - val_acc: 0.9929\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0345 - val_acc: 0.9928\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0306 - val_acc: 0.9931\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0354 - val_acc: 0.9927\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0334 - val_acc: 0.9932\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0280 - val_acc: 0.9939\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0329 - val_acc: 0.9932\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0340 - val_acc: 0.9928\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0333 - val_acc: 0.9930\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0314 - val_acc: 0.9931\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0397 - val_acc: 0.9922\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0301 - val_acc: 0.9935\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0263 - val_acc: 0.9934\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0267 - val_acc: 0.9937\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0312 - val_acc: 0.9933\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0307 - val_acc: 0.9928\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0296 - val_acc: 0.9933\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0349 - val_acc: 0.9927\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0359 - val_acc: 0.9934\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0285 - val_acc: 0.9929\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0328 - val_acc: 0.9930\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0338 - val_acc: 0.9933\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0339 - val_acc: 0.9916\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0327 - val_acc: 0.9931\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0289 - val_acc: 0.9930\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0334 - val_acc: 0.9926\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0297 - val_acc: 0.9935\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0320 - val_acc: 0.9934\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0297 - val_acc: 0.9938\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0285 - val_acc: 0.9940\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0305 - val_acc: 0.9939\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0301 - val_acc: 0.9944\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0320 - val_acc: 0.9939\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0331 - val_acc: 0.9937\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0333 - val_acc: 0.9928\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0297 - val_acc: 0.9938\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0372 - val_acc: 0.9923\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0296 - val_acc: 0.9939\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0313 - val_acc: 0.9936\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0366 - val_acc: 0.9926\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0334 - val_acc: 0.9932\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0333 - val_acc: 0.9932\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0366 - val_acc: 0.9926\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0307 - val_acc: 0.9935\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0354 - val_acc: 0.9936\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0326 - val_acc: 0.9934\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0312 - val_acc: 0.9940\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0323 - val_acc: 0.9935\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0344 - val_acc: 0.9931\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0291 - val_acc: 0.9937\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0343 - val_acc: 0.9929\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0341 - val_acc: 0.9928\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0397 - val_acc: 0.9922\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0327 - val_acc: 0.9935\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0369 - val_acc: 0.9931\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0365 - val_acc: 0.9930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0320 - val_acc: 0.9936\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0326 - val_acc: 0.9937\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0341 - val_acc: 0.9938\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0321 - val_acc: 0.9931\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0315 - val_acc: 0.9936\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0266 - val_acc: 0.9939\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0277 - val_acc: 0.9942\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0291 - val_acc: 0.9940\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0265 - val_acc: 0.9941\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0391 - val_acc: 0.9932\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0323 - val_acc: 0.9939\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0340 - val_acc: 0.9939\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0380 - val_acc: 0.9929\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0373 - val_acc: 0.9929\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0333 - val_acc: 0.9931\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0347 - val_acc: 0.9929\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0325 - val_acc: 0.9933\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0309 - val_acc: 0.9943\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0346 - val_acc: 0.9936\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0325 - val_acc: 0.9926\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0308 - val_acc: 0.9936\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0330 - val_acc: 0.9939\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0315 - val_acc: 0.9940\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0309 - val_acc: 0.9937\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0355 - val_acc: 0.9932\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0345 - val_acc: 0.9948\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0299 - val_acc: 0.9935\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0327 - val_acc: 0.9933\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0301 - val_acc: 0.9941\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0293 - val_acc: 0.9947\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0343 - val_acc: 0.9934\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0351 - val_acc: 0.9934\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0304 - val_acc: 0.9939\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0347 - val_acc: 0.9938\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0370 - val_acc: 0.9934\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0387 - val_acc: 0.9936\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0349 - val_acc: 0.9938\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0344 - val_acc: 0.9944\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0317 - val_acc: 0.9946\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0327 - val_acc: 0.9940\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0377 - val_acc: 0.9927\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0338 - val_acc: 0.9940\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0332 - val_acc: 0.9933\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0334 - val_acc: 0.9933\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0311 - val_acc: 0.9935\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0396 - val_acc: 0.9928\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0333 - val_acc: 0.9931\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0374 - val_acc: 0.9936\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0353 - val_acc: 0.9933\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0391 - val_acc: 0.9927\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0369 - val_acc: 0.9930\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0317 - val_acc: 0.9938\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0360 - val_acc: 0.9929\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0310 - val_acc: 0.9941\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0390 - val_acc: 0.9935\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0363 - val_acc: 0.9932\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0341 - val_acc: 0.9935\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0336 - val_acc: 0.9943\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0337 - val_acc: 0.9928\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0341 - val_acc: 0.9937\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0389 - val_acc: 0.9926\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0362 - val_acc: 0.9929\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0313 - val_acc: 0.9933\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0315 - val_acc: 0.9935\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0341 - val_acc: 0.9935\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0327 - val_acc: 0.9935\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0332 - val_acc: 0.9944\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0407 - val_acc: 0.9924\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0371 - val_acc: 0.9929\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0359 - val_acc: 0.9939\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0335 - val_acc: 0.9942\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0306 - val_acc: 0.9938\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0371 - val_acc: 0.9931\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0311 - val_acc: 0.9936\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0322 - val_acc: 0.9941\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0311 - val_acc: 0.9935\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0314 - val_acc: 0.9940\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0321 - val_acc: 0.9933\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0323 - val_acc: 0.9943\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0343 - val_acc: 0.9934\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0266 - val_acc: 0.9937\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0309 - val_acc: 0.9932\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0294 - val_acc: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15aac978>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 99.45%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nacc: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a174ac8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a153198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_pred = cv2.imread('um.png',  0)\n",
    "plt.imshow(img_pred,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 com probabilidade de 100.00%\n"
     ]
    }
   ],
   "source": [
    "img_pred = cv2.imread('um.png',  0)\n",
    "# força a imagem a ter as dimensões de entrada iguais às usadas nos dados de treino (28x28)\n",
    "if img_pred.shape != [28,28]:\n",
    "    img2 = cv2.resize(img_pred,(28,28))\n",
    "    img_pred = img2.reshape(28,28,-1);\n",
    "else:\n",
    "    img_pred = img_pred.reshape(28,28,-1);\n",
    "    \n",
    "# aqui também informamos o valor para a profundidade = 1, número de linhas e colunas, que correspondem 28x28 da imagem.\n",
    "img_pred = img_pred.reshape(1,1,28,28)\n",
    "pred = model.predict_classes(img_pred)\n",
    "pred_proba = model.predict_proba(img_pred)\n",
    "pred_proba = \"%.2f%%\" % (pred_proba[0][pred]*100)\n",
    "print(pred[0], \"com probabilidade de\", pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 com probabilidade de 100.00%\n"
     ]
    }
   ],
   "source": [
    "img_pred = cv2.imread('dois.png',  0)\n",
    "# força a imagem a ter as dimensões de entrada iguais às usadas nos dados de treino (28x28)\n",
    "if img_pred.shape != [28,28]:\n",
    "    img2 = cv2.resize(img_pred,(28,28))\n",
    "    img_pred = img2.reshape(28,28,-1);\n",
    "else:\n",
    "    img_pred = img_pred.reshape(28,28,-1);\n",
    "    \n",
    "# aqui também informamos o valor para a profundidade = 1, número de linhas e colunas, que correspondem 28x28 da imagem.\n",
    "img_pred = img_pred.reshape(1,1,28,28)\n",
    "pred = model.predict_classes(img_pred)\n",
    "pred_proba = model.predict_proba(img_pred)\n",
    "pred_proba = \"%.2f%%\" % (pred_proba[0][pred]*100)\n",
    "print(pred[0], \"com probabilidade de\", pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 com probabilidade de 100.00%\n"
     ]
    }
   ],
   "source": [
    "img_pred = cv2.imread('quatro.png',  0)\n",
    "# força a imagem a ter as dimensões de entrada iguais às usadas nos dados de treino (28x28)\n",
    "if img_pred.shape != [28,28]:\n",
    "    img2 = cv2.resize(img_pred,(28,28))\n",
    "    img_pred = img2.reshape(28,28,-1);\n",
    "else:\n",
    "    img_pred = img_pred.reshape(28,28,-1);\n",
    "    \n",
    "# aqui também informamos o valor para a profundidade = 1, número de linhas e colunas, que correspondem 28x28 da imagem.\n",
    "img_pred = img_pred.reshape(1,1,28,28)\n",
    "pred = model.predict_classes(img_pred)\n",
    "pred_proba = model.predict_proba(img_pred)\n",
    "pred_proba = \"%.2f%%\" % (pred_proba[0][pred]*100)\n",
    "print(pred[0], \"com probabilidade de\", pred_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 com probabilidade de 100.00%\n"
     ]
    }
   ],
   "source": [
    "img_pred = cv2.imread('cinco.png',  0)\n",
    "# força a imagem a ter as dimensões de entrada iguais às usadas nos dados de treino (28x28)\n",
    "if img_pred.shape != [28,28]:\n",
    "    img2 = cv2.resize(img_pred,(28,28))\n",
    "    img_pred = img2.reshape(28,28,-1);\n",
    "else:\n",
    "    img_pred = img_pred.reshape(28,28,-1);\n",
    "    \n",
    "# aqui também informamos o valor para a profundidade = 1, número de linhas e colunas, que correspondem 28x28 da imagem.\n",
    "img_pred = img_pred.reshape(1,1,28,28)\n",
    "pred = model.predict_classes(img_pred)\n",
    "pred_proba = model.predict_proba(img_pred)\n",
    "pred_proba = \"%.2f%%\" % (pred_proba[0][pred]*100)\n",
    "print(pred[0], \"com probabilidade de\", pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 com probabilidade de 100.00%\n"
     ]
    }
   ],
   "source": [
    "img_pred = cv2.imread('seis.png',  0)\n",
    "# força a imagem a ter as dimensões de entrada iguais às usadas nos dados de treino (28x28)\n",
    "if img_pred.shape != [28,28]:\n",
    "    img2 = cv2.resize(img_pred,(28,28))\n",
    "    img_pred = img2.reshape(28,28,-1);\n",
    "else:\n",
    "    img_pred = img_pred.reshape(28,28,-1);\n",
    "    \n",
    "# aqui também informamos o valor para a profundidade = 1, número de linhas e colunas, que correspondem 28x28 da imagem.\n",
    "img_pred = img_pred.reshape(1,1,28,28)\n",
    "pred = model.predict_classes(img_pred)\n",
    "pred_proba = model.predict_proba(img_pred)\n",
    "pred_proba = \"%.2f%%\" % (pred_proba[0][pred]*100)\n",
    "print(pred[0], \"com probabilidade de\", pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
